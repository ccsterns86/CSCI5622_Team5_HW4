{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# CSCI 5622\n",
    "## Homework 4: Designing ML Models for Real-World Problems\n",
    "### Team 5 - Study 2\n",
    "##### Cassie Sterns, Saksham Khatwani, Jasdeep Singh, and Nirmit Karkera"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statistics as stat\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading in data\n",
    "prosodic_file = \"prosodic_features.csv\"\n",
    "scores_file = \"scores.csv\"\n",
    "transcript_file = \"transcripts.csv\"\n",
    "\n",
    "# Read csvs into a file\n",
    "prosodic_data = pd.read_csv(prosodic_file)\n",
    "scores_data = pd.read_csv(scores_file)\n",
    "transcript_data = pd.read_csv(transcript_file)\n",
    "\n",
    "# Add a column that corresponds to participant for easy splitting for prosodic data\n",
    "prosodic_data['Participant'] = prosodic_data['participant&question'].str.extract(r'^(PP?\\d+)')\n",
    "prosodic_data['Participant'] = prosodic_data['Participant'].str.lower()\n",
    "# Make the entire transcript lower case\n",
    "transcript_data['Transcript'] = transcript_data['Transcript'].str.lower()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1: ['70', '49', '30', '12', '67', '15', '81', '48', '5', '14', '4', '63', '10', '79']\n",
      "\tLength:  14\n",
      "Fold 2: ['72', '83', '43', '56', '64', '84', '35', '31', '80', '71', '74', '89', '59', '53']\n",
      "\tLength:  14\n",
      "Fold 3: ['6', '62', '58', '66', '8', '32', '22', '45', '13', '25', '52', '11', '77', '29']\n",
      "\tLength:  14\n",
      "Fold 4: ['34', '42', '27', '24', '21', '78', '55', '61', '33', '57', '85', '73', '1', '16']\n",
      "\tLength:  14\n",
      "Fold 5: ['60', '17', '44', '3', '47', '20', '76', '37', '86', '69', '7', '65', '50']\n",
      "\tLength:  13\n"
     ]
    }
   ],
   "source": [
    "# Splitting the participants into 5 folds\n",
    "num_folds = 5\n",
    "\n",
    "# Grab participant numbers from the scores csv file\n",
    "interviews = scores_data['Participant'].unique()\n",
    "participants = list(set([re.sub(r'^pp?|q\\d+', '', item) for item in interviews]))\n",
    "random.shuffle(participants)\n",
    "participant_folds = [participants[i::num_folds] for i in range(num_folds)]\n",
    "\n",
    "for i, fold in enumerate(participant_folds):\n",
    "  print(f\"Fold {i + 1}: {fold}\")\n",
    "  print(\"\\tLength: \", len(fold))\n",
    "\n",
    "# Grab all the correct interview names associated with each participant\n",
    "interview_folds = []\n",
    "for fold in participant_folds:\n",
    "  interview_folds.append([item for num in fold for item in (f\"p{num}\", f\"pp{num}\")])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_splits(data, fold_number):\n",
    "    \"\"\"\n",
    "    Split data into training, validation, and testing sets based on a specified fold.\n",
    "    \n",
    "    Parameters:\n",
    "        data (DataFrame): The complete dataset.\n",
    "        fold_number (int): The fold to use for testing (0-based index).\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (training_set, validation_set, testing_set)\n",
    "    \"\"\"\n",
    "    # Quick check on fold number\n",
    "    assert 0 <= fold_number < len(interview_folds), \"Fold_number must be between 0 and len(folds) - 1\"\n",
    "\n",
    "    # Split the data\n",
    "    test_set = data[data['Participant'].isin( interview_folds[fold_number] )]\n",
    "    val_set = data[data['Participant'].isin( interview_folds[(fold_number + 1) % len(interview_folds)] )]\n",
    "    train_set_parts = [item for i, fold in enumerate(interview_folds) if i not in [fold_number, (fold_number + 1) % len(interview_folds)] for item in fold]\n",
    "    train_set = data[data['Participant'].isin(train_set_parts)]\n",
    "    \n",
    "    return train_set, val_set, test_set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, val_set, test_set = get_data_splits(prosodic_data, 0)\n",
    "# print(\"Prosodic Data: \")\n",
    "# train_set.head()\n",
    "train_set, val_set, test_set = get_data_splits(scores_data, 0)\n",
    "# print(\"Scores Data: \")\n",
    "# test_set.head()\n",
    "train_set, val_set, test_set = get_data_splits(transcript_data, 0)\n",
    "# print(\"Transcript Data: \")\n",
    "# test_set.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (a) Extracting language features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\ccste\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     C:\\Users\\ccste\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\ccste\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "nltk.download('punkt_tab')  # Tokenizer\n",
    "nltk.download('averaged_perceptron_tagger_eng')  # POS Tagger\n",
    "nltk.download('vader_lexicon') # Vader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Syntactic Vectorization with CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>000</th>\n",
       "      <th>10</th>\n",
       "      <th>100</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>18</th>\n",
       "      <th>20</th>\n",
       "      <th>200</th>\n",
       "      <th>...</th>\n",
       "      <th>yep</th>\n",
       "      <th>yes</th>\n",
       "      <th>yo</th>\n",
       "      <th>yoga</th>\n",
       "      <th>york</th>\n",
       "      <th>young</th>\n",
       "      <th>younger</th>\n",
       "      <th>youngest</th>\n",
       "      <th>yup</th>\n",
       "      <th>zone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 2464 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   000  10  100  13  14  15  16  18  20  200  ...  yep  yes  yo  yoga  york  \\\n",
       "0    1   0    0   0   0   0   0   0   0    0  ...    0    1   0     0     0   \n",
       "1    0   0    0   0   0   0   0   0   0    0  ...    0    0   0     0     0   \n",
       "2    0   0    0   0   0   0   0   0   0    0  ...    0    0   0     0     0   \n",
       "3    0   0    0   0   0   0   0   0   0    0  ...    0    0   0     0     0   \n",
       "4    0   0    0   0   0   0   0   0   0    0  ...    0    0   0     0     0   \n",
       "\n",
       "   young  younger  youngest  yup  zone  \n",
       "0      0        0         0    0     0  \n",
       "1      0        0         0    0     0  \n",
       "2      0        0         0    0     0  \n",
       "3      0        0         0    0     0  \n",
       "4      0        0         0    0     0  \n",
       "\n",
       "[5 rows x 2464 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Syntactic vectorizer: CountVectorizer\n",
    "\n",
    "# Remove common stop words in english and ignore words that appear fewer than 2 times\n",
    "vectorizer = CountVectorizer(stop_words='english', min_df=2) \n",
    "X = vectorizer.fit_transform(transcript_data['Transcript'])\n",
    "\n",
    "# Convert from sparse X matrix to a denser one for easy use\n",
    "X_dense = X.toarray()\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "word_count = pd.DataFrame(X_dense, columns=feature_names)\n",
    "\n",
    "word_count.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Syntactic Vectorization with TFIDF Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>000</th>\n",
       "      <th>10</th>\n",
       "      <th>100</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>18</th>\n",
       "      <th>20</th>\n",
       "      <th>200</th>\n",
       "      <th>...</th>\n",
       "      <th>yep</th>\n",
       "      <th>yes</th>\n",
       "      <th>yo</th>\n",
       "      <th>yoga</th>\n",
       "      <th>york</th>\n",
       "      <th>young</th>\n",
       "      <th>younger</th>\n",
       "      <th>youngest</th>\n",
       "      <th>yup</th>\n",
       "      <th>zone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.068513</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.031465</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 2464 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        000   10  100   13   14   15   16   18   20  200  ...  yep       yes  \\\n",
       "0  0.068513  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.031465   \n",
       "1  0.000000  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.000000   \n",
       "2  0.000000  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.000000   \n",
       "3  0.000000  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.000000   \n",
       "4  0.000000  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.000000   \n",
       "\n",
       "    yo  yoga  york  young  younger  youngest  yup  zone  \n",
       "0  0.0   0.0   0.0    0.0      0.0       0.0  0.0   0.0  \n",
       "1  0.0   0.0   0.0    0.0      0.0       0.0  0.0   0.0  \n",
       "2  0.0   0.0   0.0    0.0      0.0       0.0  0.0   0.0  \n",
       "3  0.0   0.0   0.0    0.0      0.0       0.0  0.0   0.0  \n",
       "4  0.0   0.0   0.0    0.0      0.0       0.0  0.0   0.0  \n",
       "\n",
       "[5 rows x 2464 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Syntactic vectorizer: TFIDF Vectorizer\n",
    "tfidf = TfidfVectorizer(stop_words='english', min_df=2)\n",
    "tfidf_matrix = tfidf.fit_transform(transcript_data['Transcript'])\n",
    "feature_names = tfidf.get_feature_names_out()\n",
    "tfidf_count = pd.DataFrame(tfidf_matrix.toarray(), columns=feature_names)\n",
    "\n",
    "tfidf_count.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistical Features (word count and average word length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word count</th>\n",
       "      <th>avg word length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>613</td>\n",
       "      <td>4.438825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1118</td>\n",
       "      <td>4.510733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>751</td>\n",
       "      <td>4.528628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>717</td>\n",
       "      <td>4.281729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>645</td>\n",
       "      <td>4.688372</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   word count  avg word length\n",
       "0         613         4.438825\n",
       "1        1118         4.510733\n",
       "2         751         4.528628\n",
       "3         717         4.281729\n",
       "4         645         4.688372"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Statistical Features (word count and average word length)\n",
    "\n",
    "# Word Count for the entire interview\n",
    "features = pd.DataFrame()\n",
    "features['word count'] = transcript_data['Transcript'].apply(lambda x: len(x.split()))\n",
    "\n",
    "# Average word length for the interview\n",
    "features['avg word length'] = transcript_data['Transcript'].apply(lambda x: sum(len(word) for word in x.split()) / len(x.split()))\n",
    "\n",
    "features.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part of Speech Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [(interviewer, NN), (:, :), (so, RB), (how, WR...\n",
       "1    [(interviewer, NN), (:, :), (so, RB), (how, WR...\n",
       "2    [(interviewer, NN), (:, :), (so, RB), (tell, V...\n",
       "3    [(interviewer, NN), (:, :), (so, RB), (how, WR...\n",
       "4    [(interviewer, NN), (:, :), (how, WRB), (are, ...\n",
       "Name: pos tagging, dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Part of Speech Tagging using NLTK\n",
    "\n",
    "features['pos tagging'] = transcript_data['Transcript'].apply(lambda x: pos_tag(word_tokenize(x)))\n",
    "\n",
    "features['pos tagging'].head()\n",
    "# NN: Noun singular\n",
    "# VB: Verb base form\n",
    "# JJ: Adjective\n",
    "# RB: Adverb\n",
    "# DT: Determiner\n",
    "# IN: Preposition or subordination conjunction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment Analysis with Vader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    {'neg': 0.013, 'neu': 0.859, 'pos': 0.128, 'co...\n",
       "1    {'neg': 0.026, 'neu': 0.852, 'pos': 0.122, 'co...\n",
       "2    {'neg': 0.02, 'neu': 0.874, 'pos': 0.105, 'com...\n",
       "3    {'neg': 0.027, 'neu': 0.842, 'pos': 0.131, 'co...\n",
       "4    {'neg': 0.038, 'neu': 0.838, 'pos': 0.124, 'co...\n",
       "Name: sentiment, dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sentiment Analysis with Vader\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "features['sentiment'] = transcript_data['Transcript'].apply(lambda x: analyzer.polarity_scores(x))\n",
    "\n",
    "features['sentiment'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Embeddings with BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word embeddings with BERT (Hugging Face)\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [[0.37750718, -0.63461435, -0.19307065, -0.055...\n",
       "1    [[0.40645075, -0.6960126, -0.30729404, -0.1341...\n",
       "2    [[0.35458925, -0.15315087, -0.39364684, 0.1442...\n",
       "3    [[0.34659496, -0.69377905, -0.347427, -0.00977...\n",
       "4    [[0.28002012, -0.5724305, -0.42716113, -0.1497...\n",
       "Name: word embeddings, dtype: object"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Separate interveiw into sentences\n",
    "def process_transcript(text):\n",
    "  sentences = re.split(r'(?<=\\|)', text)\n",
    "  cleaned_sentences = [\n",
    "    re.sub(r'^(interviewer:|interviewee:)\\s*', '', s.strip().replace('|', '')) for s in sentences if s.strip()\n",
    "  ]\n",
    "  return cleaned_sentences\n",
    "\n",
    "transcript_data['Processed Transcript'] = transcript_data['Transcript'].apply(process_transcript)\n",
    "\n",
    "# Tokenize the text\n",
    "def get_bert_embeddings(sentences):\n",
    "  embeddings = []\n",
    "  for sentence in sentences:\n",
    "    # Tokenize and get input Id and attention mask\n",
    "    inputs = tokenizer(sentence, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "    input_ids = inputs['input_ids']\n",
    "    attention_mask = inputs['attention_mask']\n",
    "\n",
    "    # Use BERT\n",
    "    with torch.no_grad():\n",
    "      outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "    # Pool output\n",
    "    cls_embedding = outputs.last_hidden_state.mean(dim=1).squeeze().numpy()\n",
    "    embeddings.append(cls_embedding)\n",
    "\n",
    "  return embeddings\n",
    "\n",
    "features['word embeddings'] = transcript_data['Processed Transcript'].apply(get_bert_embeddings)\n",
    "\n",
    "features['word embeddings'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Language Features Extracted: \n",
    "- Syntactic Vectorization with CountVectorizer \n",
    "- Syntactic Vectorization with TFIDF Vectorizer\n",
    "- Word Count\n",
    "- Average Word Length\n",
    "- Part of Speech Tagging\n",
    "- Sentiment Analysis with Vader\n",
    "- Word Embedding with BERT\n",
    "\n",
    "A total of 7 distinctive features. \n",
    "\n",
    "The human readable features are: CountVectorizer because it is a count of the words throughout the interview, the Statistical Features, Part of Speech Tagging (with a little extra interpretation), and Sentiment Analysis with Vader."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (b) Language feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (c) Estimating interview outcomes based on language"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (d) Multimodal ML models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (e) Explainable ML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
